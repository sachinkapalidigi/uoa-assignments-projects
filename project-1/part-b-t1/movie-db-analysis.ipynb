{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n",
      "<class 'str'>\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" class=\"no-js\">\n",
      "  <head>\n",
      "    <title>Popular Movies &#8212; The Movie Database (TMDB)</title>\n",
      "    <meta http-equiv=\"cleartype\" content=\"on\">\n",
      "    <meta charset=\"utf-8\">\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 1. Establish a connection to the webpage - \"https://www.themoviedb.org/movie\"\n",
    "import requests\n",
    "\n",
    "tmdb_url = 'https://www.themoviedb.org/movie'\n",
    "\n",
    "needed_headers = {\n",
    "  'User-Agent': \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36\"\n",
    "}\n",
    "\n",
    "page_source = ''\n",
    "\n",
    "response = requests.get(tmdb_url, headers=needed_headers) # 1a\n",
    "status_code = response.status_code\n",
    "if response.ok: # 1b\n",
    "  page_source = response.text # 1c\n",
    "  # print(page_source) # 1c\n",
    "  print(response.encoding) # 1d\n",
    "  print(type(page_source)) # 1d\n",
    "  print(page_source[:200]) # 1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting validators\n",
      "  Obtaining dependency information for validators from https://files.pythonhosted.org/packages/3a/0c/785d317eea99c3739821718f118c70537639aa43f96bfa1d83a71f68eaf6/validators-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: validators\n",
      "Successfully installed validators-0.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Popular Movies\n",
      "\n",
      "Content fetched successfully for url: https://www.scrapethissite.com/pages/simple/\n",
      "failed to fetch: Error fetching url\n"
     ]
    }
   ],
   "source": [
    "# 2. Parse the content of HTML response using the BeautifulSoup library and execute the tasks specified in the guidelines mentioned below\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import validators\n",
    "\n",
    "soup = BeautifulSoup(page_source, features=\"html.parser\")\n",
    "\n",
    "page_title = soup.find(class_='title').text\n",
    "print(page_title)\n",
    "\n",
    "\n",
    "def fetch_parse_url(url):\n",
    "  if not validators.url(url):\n",
    "    raise Exception('Invalid url')\n",
    "  \n",
    "  needed_headers = {\n",
    "    'User-Agent': \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36\"\n",
    "  }\n",
    "  \n",
    "  try:\n",
    "    response = requests.get(url, headers=needed_headers)\n",
    "    response.raise_for_status()\n",
    "  except requests.exceptions.HTTPError as e:\n",
    "    raise Exception(f\"HTTP Error: {e.response.status_code}\") from e\n",
    "  except requests.exceptions.RequestException as e:\n",
    "    raise Exception(\"Error fetching url\") from e\n",
    "  else:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "working_url = \"https://www.scrapethissite.com/pages/simple/\"\n",
    "try:\n",
    "  result = fetch_parse_url(working_url)\n",
    "  print(f\"Content fetched successfully for url: {working_url}\")\n",
    "except Exception as e:\n",
    "  print(f\"failed to fetch: {e}\")\n",
    "\n",
    "broken_url = \"https://www.cannotscrapethissite.com/\"\n",
    "try:\n",
    "  result = fetch_parse_url(broken_url)\n",
    "  print(f\"Content fetched successfully for url: {broken_url}\")\n",
    "except Exception as e:\n",
    "  print(f\"failed to fetch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract content from TBDB website\n",
    "tmdb_content_soup = None\n",
    "try:\n",
    "  tmdb_content_soup = fetch_parse_url(tmdb_url) # 3a\n",
    "except Exception as e:\n",
    "  print(f\"Cannot fetch TMDB content: {e}\")\n",
    "\n",
    "# not printing as html output is too long\n",
    "# print(tmdb_content_soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"card style_1\">\n",
      " <div class=\"image\">\n",
      "  <div class=\"wrapper glyphicons_v2 picture grey no_image_holder\">\n",
      "   <a class=\"image\" href=\"/movie/1072790\" title=\"Anyone But You\">\n",
      "    <img alt=\"Anyon\n",
      "------- First movie title --------\n",
      "Anyone But You\n",
      "------- First movie rating --------\n",
      "68.94999999999999\n",
      "------- First movie path --------\n",
      "/movie/1072790\n"
     ]
    }
   ],
   "source": [
    "# 3 Extract from TMDB website continued...\n",
    "\n",
    "movie_containers = tmdb_content_soup.find(id=\"media_results\").find_all(\"div\", class_=\"card style_1\")\n",
    "first_movie_container = None\n",
    "if len(movie_containers) > 0:\n",
    "  first_movie_container = movie_containers[0]\n",
    "  print(first_movie_container.prettify()[:200]) # 3b printing only 200 characters as html output is too long\n",
    "else:\n",
    "  # print(\"Could not find the first movie container\")\n",
    "  raise Exception(\"Could not find the first movie container\")\n",
    "\n",
    "try:\n",
    "  first_movie_content = first_movie_container.find(\"div\", class_=\"content\")\n",
    "  first_movie_title = first_movie_content.find('h2').find('a').text\n",
    "  print(\"------- First movie title --------\")\n",
    "  print(first_movie_title) # 3c\n",
    "except Exception as e:\n",
    "  raise Exception(\"Could not find the first movie title\")\n",
    "\n",
    "try:\n",
    "  first_movie_rating = first_movie_content.find(\"div\", class_=\"user_score_chart\").get(\"data-percent\")\n",
    "  print(\"------- First movie rating --------\")\n",
    "  print(first_movie_rating) # 3d\n",
    "except Exception as e:\n",
    "  raise Exception(\"Could not find the first movie rating\")\n",
    "\n",
    "try:\n",
    "  first_movie_content_2 = first_movie_container.find(\"div\", class_=\"content\")\n",
    "  first_movie_path = first_movie_content_2.find('h2').find('a').get('href')\n",
    "  print(\"------- First movie path --------\")\n",
    "  print(first_movie_path) # 3e\n",
    "except Exception as e:\n",
    "  raise Exception(\"Could not find the first movie path\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write user defined functions\n",
    "import time\n",
    "\n",
    "def is_float(s):\n",
    "  try:\n",
    "    float(s)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "# 4a\n",
    "def get_titles(page_soup):\n",
    "  titles = []\n",
    "  try:\n",
    "    elements = page_soup.find(id=\"media_results\").find_all(\"div\", class_=\"card style_1\")\n",
    "    for element in elements:\n",
    "      titles.append(element.find(\"div\", class_=\"content\").find('h2').find('a')['title'])\n",
    "  except Exception as e:\n",
    "    raise Exception(\"Couldnot get movie titles\") from e\n",
    "  return titles\n",
    "\n",
    "# 4b\n",
    "def get_user_ratings(page_soup):\n",
    "  ratings = []\n",
    "  try:\n",
    "    elements = page_soup.find(id=\"media_results\").find_all(\"div\", class_=\"card style_1\")\n",
    "    for element in elements:\n",
    "      rating = element.find(\"div\", class_=\"user_score_chart\").get(\"data-percent\", 'n/a')\n",
    "      if is_float(rating):\n",
    "        ratings.append(rating)\n",
    "      else:\n",
    "        ratings.append(\"not rated\")\n",
    "  except Exception as e:\n",
    "    raise Exception(\"Couldnot get movie ratings\") from e\n",
    "  return ratings\n",
    "\n",
    "# 4c\n",
    "def get_movie_urls(page_soup):\n",
    "  urls = []\n",
    "  try:\n",
    "    elements = page_soup.find(id=\"media_results\").find_all(\"div\", class_=\"card style_1\")\n",
    "    for element in elements:\n",
    "      urls.append(element.find(\"div\", class_=\"content\").find('h2').find('a').get('href'))\n",
    "  except Exception as e:\n",
    "    raise Exception(\"Could not get movie URLs\") from e\n",
    "  return urls\n",
    "\n",
    "def get_movie_pages(urls):\n",
    "  page_soups = []\n",
    "  base_url = 'https://www.themoviedb.org'\n",
    "  try:\n",
    "    for url in urls:\n",
    "      page_soup = fetch_parse_url(base_url + url)\n",
    "      page_soups.append(page_soup)\n",
    "      time.sleep(2)\n",
    "  except Exception as e:\n",
    "    raise Exception('Could not get movie pages')\n",
    "  return page_soups\n",
    "\n",
    "# 4d - modified for efficiency\n",
    "def get_movies_genres(page_soups):\n",
    "  genres = []\n",
    "  try:\n",
    "    for page_soup in page_soups:\n",
    "      movie_genres_tags = page_soup.find('span', class_='genres').find_all('a')\n",
    "      movie_genres = list(map(lambda x: x.text, movie_genres_tags))\n",
    "      genres.append(movie_genres)\n",
    "  except Exception as e:\n",
    "    raise Exception(\"Could not get genres\") from e\n",
    "  return genres\n",
    "\n",
    "# 4e - modified for efficiency\n",
    "def get_movies_cast(page_soups):\n",
    "  movies_cast = []\n",
    "  try:\n",
    "    for page_soup in page_soups:\n",
    "      movie_cast = []\n",
    "      cast_section = page_soup.find(id='cast_scroller').find('ol', class_='people scroller').find_all('li', class_='card')\n",
    "      for cast_card in cast_section:\n",
    "        movie_cast.append(cast_card.find('p').text)\n",
    "      movies_cast.append(movie_cast)\n",
    "  except Exception as e:\n",
    "    raise Exception(\"Could not get movies cast\") from e\n",
    "  return movies_cast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/r6_kjvss0hq6xpg6frkjsxk80000gn/T/ipykernel_74254/3177981433.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# 5. Write an user defined function that returns a pandas data frame with following data\n",
    "import pandas as pd\n",
    "\n",
    "def get_movies_df(page_soup):\n",
    "  data_frame = pd.DataFrame()\n",
    "  data_frame['title'] = get_titles(page_soup)\n",
    "  data_frame['rating'] = get_user_ratings(page_soup)\n",
    "  movie_pages = get_movie_pages(get_movie_urls(page_soup))\n",
    "  data_frame['genres'] = get_movies_genres(movie_pages)\n",
    "  data_frame['cast'] = get_movies_cast(movie_pages)\n",
    "  return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Scraping the data and combining the dataframes\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Save the data frame to a csv file, append if already exists\n",
    "def save_df(file_path, df):\n",
    "  file_exists = os.path.isfile(file_path)\n",
    "  df.to_csv(file_path, mode='a', header=not  file_exists)\n",
    "\n",
    "# Fetch the movies page, based on number and base url. Save each extracted data to a file within a folder\n",
    "def scrape_save_movies_page(base_url, dir_path, base_csv_name, page):\n",
    "  url = base_url\n",
    "  if page != 1:\n",
    "    url = f\"{base_url}?page={page}\"\n",
    "  page_soup = fetch_parse_url(url)\n",
    "  df = get_movies_df(page_soup)\n",
    "  if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "  save_df(f\"{dir_path}/{base_csv_name}_{page}.csv\", df)\n",
    "  return df\n",
    "\n",
    "# Fetch for page 1 to 5 by calling the scrape_save_movies_page, concat the data frames into a single data frame and return.\n",
    "def scrape_movie_pages(base_url, dir_path):\n",
    "  dfs = []\n",
    "  for i in range(1, 6):\n",
    "    df = scrape_save_movies_page(base_url, dir_path, 'tmtb_db', i)\n",
    "    dfs.append(df)\n",
    "  return pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "base_url = 'https://www.themoviedb.org/movie'\n",
    "dir_path = './movies_data'\n",
    "movie_df = scrape_movie_pages(base_url, dir_path)\n",
    "\n",
    "# combined csv\n",
    "save_df(\"tmtb_db_all.csv\", movie_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            title rating                                          genres  \\\n",
      "0     Land of Bad  71.99                         [Action, Thriller, War]   \n",
      "1  Anyone But You  68.92                               [Comedy, Romance]   \n",
      "2       Migration  76.38  [Animation, Action, Adventure, Comedy, Family]   \n",
      "3     The Marvels  62.56            [Science Fiction, Adventure, Action]   \n",
      "4       No Way Up   57.0                      [Action, Horror, Thriller]   \n",
      "\n",
      "                                                cast  \n",
      "0  [Liam Hemsworth, Russell Crowe, Luke Hemsworth...  \n",
      "1  [Sydney Sweeney, Glen Powell, Alexandra Shipp,...  \n",
      "2  [Kumail Nanjiani, Elizabeth Banks, Caspar Jenn...  \n",
      "3  [Brie Larson, Teyonah Parris, Iman Vellani, Za...  \n",
      "4  [Sophie McIntosh, Will Attenborough, Jeremias ...  \n"
     ]
    }
   ],
   "source": [
    "print(movie_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              title rating              genres  \\\n",
      "count           100    100                 100   \n",
      "unique          100     94                  78   \n",
      "top     Land of Bad   61.0  [Action, Thriller]   \n",
      "freq              1      3                   5   \n",
      "\n",
      "                                                     cast  \n",
      "count                                                 100  \n",
      "unique                                                100  \n",
      "top     [Liam Hemsworth, Russell Crowe, Luke Hemsworth...  \n",
      "freq                                                    1  \n"
     ]
    }
   ],
   "source": [
    "print(movie_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   100 non-null    object\n",
      " 1   rating  100 non-null    object\n",
      " 2   genres  100 non-null    object\n",
      " 3   cast    100 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(movie_df.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
